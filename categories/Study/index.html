<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>카테고리: Study - ikarus&#039;s BLOG</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="ikarus&#039;s BLOG"><meta name="msapplication-TileImage" content="img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="ikarus&#039;s BLOG"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="이카루스의 블로그입니다."><meta property="og:type" content="blog"><meta property="og:title" content="ikarus&#039;s BLOG"><meta property="og:url" content="https://ikarus-999.github.io/"><meta property="og:site_name" content="ikarus&#039;s BLOG"><meta property="og:description" content="이카루스의 블로그입니다."><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://ikarus-999.github.io/img/og_image.png"><meta property="article:author" content="ikarus-999"><meta property="article:tag" content="Deep Learning, Sound"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://ikarus-999.github.io"},"headline":"ikarus's BLOG","image":["https://ikarus-999.github.io/img/og_image.png"],"author":{"@type":"Person","name":"ikarus-999"},"publisher":{"@type":"Organization","name":"ikarus's BLOG","logo":{"@type":"ImageObject","url":"https://ikarus-999.github.io/img/logo.svg"}},"description":"이카루스의 블로그입니다."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-216914004-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-216914004-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/rss2.xml" title="ikarus's BLOG" type="application/rss+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="ikarus&#039;s BLOG" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">카테고리</a></li><li class="is-active"><a href="#" aria-current="page">Study</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-09T12:34:32.000Z" title="2021. 9. 9. 오후 9:34:32">2021-09-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:20:36.820Z" title="2022. 4. 5. 오후 3:20:36">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Competition/">Competition</a></span><span class="level-item">3분안에 읽기 (약 414 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/">인공지능 경진대회 후기_2021</a></h1><div class="content"><h2 id="인공지능-경진대회-참가후기-2탄"><a href="#인공지능-경진대회-참가후기-2탄" class="headerlink" title="인공지능 경진대회 참가후기 2탄"></a>인공지능 경진대회 참가후기 2탄</h2><p>  링크 1 : <a target="_blank" rel="noopener" href="http://www.aitimes.kr/news/articleView.html?idxno=21647">http://www.aitimes.kr/news/articleView.html?idxno=21647</a><br>  링크 2 : <a target="_blank" rel="noopener" href="http://www.aitimes.com/news/articleView.html?idxno=139557">http://www.aitimes.com/news/articleView.html?idxno=139557</a>  </p>
<p>  팀명은 자연소프트</p>
<p>  이번엔 협업을 하면서 진행하였다</p>
<p>  처음까진 페이스 괜찮았다가</p>
<p>  진행하는 내내 순위가 정말 짜릿했었다</p>
<p>  <img src="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/%EC%99%80%EC%9B%88_2.png" alt="와웈"></p>
<p>  Image Segmentation 대회여서 이미지 윤곽선 찾는 모델인</p>
<p>  Unet, EfficientNet 계열을 찾아보았다.  </p>
<p>  IOU 를 높이기 위해(이미지 윤곽선 찾는 알고리즘&#x3D;&gt;음성 분리에도 쓰일듯)  </p>
<p>  Unet 계열 모델도 약간 좋은 Iou가 나오지만 </p>
<p>  주최측 베이스라인 코드에서 엄청난 함정을 또 발견하였다!!  </p>
<p>  정답 마스크의 손실압축된 값으로 iou 계산 부분 이었다<br>  베이스라인 코드는 PIL Image는 손실 압축이였…<br>  빨리 npy 무손실 압축 포맷으로 바꿈.  </p>
<p>  하지만 Mask -&gt; Polygon 알고리즘 할때 COntour 갯수 리밋이 있어서 중요한 컨투어만 남기는 것과, 데이터 증강 방법을 고민하고 개발했었다. </p>
<p>  EfficientNet B4 모델에 Data Augmentation, Annotation 방법을 발견했었다.<br>  <img src="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/%EC%8B%A4%ED%97%98%ED%96%88%EB%8D%98%EB%AA%A8%EB%8D%B8.png" alt="텐서플로_수식_코딩">  </p>
<p>  점수 향상된 데이터 증강.. 랜덤 변수를 조금만 더 cliping 했으면 마스크 이미지 잘 보여서 좋았을텐데..<br>  <img src="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/DataAug_Segmentation.png" alt="DataAug_Segmentation">  </p>
<p>  중간 스코어(public score)<br>  <img src="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/%EB%82%98%EC%9D%98%EA%B2%B0%EA%B3%BC_%EC%B5%9C%EC%B4%88.png" alt="헐..">
  </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-06T05:54:41.000Z" title="2021. 9. 6. 오후 2:54:41">2021-09-06</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:22:02.092Z" title="2022. 4. 5. 오후 3:22:02">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">9분안에 읽기 (약 1410 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/06/Multi-GPU/">Multi-GPU</a></h1><div class="content"><h2 id="작곡-GAN-x-Multi-GPU"><a href="#작곡-GAN-x-Multi-GPU" class="headerlink" title="작곡 GAN x Multi-GPU"></a>작곡 GAN x Multi-GPU</h2><h3 id="작곡-GAN-만들어보기"><a href="#작곡-GAN-만들어보기" class="headerlink" title="작곡 GAN 만들어보기"></a>작곡 GAN 만들어보기</h3><ol>
<li>음악 작곡 전처리 방법 : midi파일을 입출력으로 사용</li>
<li>pypianoroll로 각 트랙의 악기, 키, 코드, 화음 계산 반영</li>
</ol>
<h3 id="네트워크-구조"><a href="#네트워크-구조" class="headerlink" title="네트워크 구조"></a>네트워크 구조</h3><ol>
<li><p>Generator<br>  input : Single Melody track (Timestep, n_pitch, n_tracks)<br>  Latent noise Vector z: (2, 8, 512)<br>  U-Net 구조로 되어있음.<br>  <img src="/2021/09/06/Multi-GPU/%EB%A9%80%ED%8B%B0%EC%A7%80%ED%93%A8_01.png" alt="U-net">     </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">def</span> <span class="title function_">_conv2d</span>(<span class="params">layer_input, filters, f_size=<span class="number">4</span>, bn=<span class="literal">True</span></span>):</span><br><span class="line">      <span class="comment"># 다운샘플링</span></span><br><span class="line">      d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">&#x27;same&#x27;</span>)(layer_input)</span><br><span class="line">      d = keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)(d)</span><br><span class="line">      <span class="keyword">if</span> bn:</span><br><span class="line">          d = keras.layers.BatchNormalization(momentum=<span class="number">0.8</span>)(d)</span><br><span class="line">      <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_deconv2d</span>(<span class="params">layer_input, pre_input, filters, f_size=<span class="number">4</span>, dropout_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="comment"># 업 샘플링</span></span><br><span class="line">        u = keras.layers.UpSampling2D(size=<span class="number">2</span>)(layer_input)</span><br><span class="line">        u = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">1</span>,</span><br><span class="line">                                  padding=<span class="string">&#x27;same&#x27;</span>)(u)</span><br><span class="line">        u = keras.layers.BatchNormalization(momentum=<span class="number">0.8</span>)(u)</span><br><span class="line">        u = keras.layers.ReLU()(u)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dropout_rate:</span><br><span class="line">            u = keras.layers.Dropout(dropout_rate)(u)</span><br><span class="line">            </span><br><span class="line">        u = keras.layers.Concatenate()([u, pre_input])</span><br><span class="line">        <span class="keyword">return</span> u</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_generator</span>(<span class="params">condition_input_shape=(<span class="params"><span class="number">32</span>, <span class="number">128</span>, <span class="number">1</span></span>), filters=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                        instruments=<span class="number">4</span>, latent_shape=(<span class="params"><span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 제네레이터 블록</span></span><br><span class="line">        c_input = keras.layers.Input(shape=condition_input_shape)</span><br><span class="line">        z_input = keras.layers.Input(shape=latent_shape)</span><br><span class="line"></span><br><span class="line">        d1 = _conv2d(c_input, filters, bn=<span class="literal">False</span>)</span><br><span class="line">        d2 = _conv2d(d1, filters * <span class="number">2</span>)</span><br><span class="line">        d3 = _conv2d(d2, filters * <span class="number">4</span>)</span><br><span class="line">        d4 = _conv2d(d3, filters * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        d4 = keras.layers.Concatenate(axis=-<span class="number">1</span>)([d4, z_input])</span><br><span class="line"></span><br><span class="line">        u4 = _deconv2d(d4, d3, filters * <span class="number">4</span>)</span><br><span class="line">        u5 = _deconv2d(u4, d2, filters * <span class="number">2</span>)</span><br><span class="line">        u6 = _deconv2d(u5, d1, filters)</span><br><span class="line"></span><br><span class="line">        u7 = keras.layers.UpSampling2D(size=<span class="number">2</span>)(u6)</span><br><span class="line">        output = keras.layers.Conv2D(instruments, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>,</span><br><span class="line">                                  padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;tanh&#x27;</span>)(u7)  <span class="comment"># 32, 128, 4</span></span><br><span class="line"></span><br><span class="line">        generator = keras.models.Model([c_input, z_input], output, name=<span class="string">&#x27;Generator&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> generator</span><br><span class="line">    ```  </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Discriminator  </span><br><span class="line">  ![U-net](/2021/09/06/Multi-GPU/멀티지퓨_03.png)  </span><br><span class="line">    </span><br><span class="line">    ```python</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_discriminator_layer</span>(<span class="params">layer_input, filters, f_size=<span class="number">4</span></span>):</span><br><span class="line">      </span><br><span class="line">          <span class="comment"># input:  [batch_size, in_channels, H, W]</span></span><br><span class="line">          <span class="comment"># output: [batch_size, out_channels, H/2, W/2]</span></span><br><span class="line">      </span><br><span class="line">      d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">&#x27;same&#x27;</span>)(layer_input)</span><br><span class="line">      <span class="comment"># Discriminator는 BatchNorm을 쓰지 않습니다!!</span></span><br><span class="line">      d = keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)(d) </span><br><span class="line">      <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_discriminator</span>(<span class="params">pianoroll_shape=(<span class="params"><span class="number">32</span>, <span class="number">128</span>, <span class="number">4</span></span>), filters=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># WGAN Discriminator(비평자)</span></span><br><span class="line">        </span><br><span class="line">        condition_input_shape = (<span class="number">32</span>,<span class="number">128</span>,<span class="number">1</span>)</span><br><span class="line">        groundtruth_pianoroll = keras.layers.Input(shape=pianoroll_shape)</span><br><span class="line">        condition_input = keras.layers.Input(shape=condition_input_shape)</span><br><span class="line">        combined_imgs = keras.layers.Concatenate(axis=-<span class="number">1</span>)([groundtruth_pianoroll, condition_input])</span><br><span class="line"></span><br><span class="line">        d1 = _build_discriminator_layer(combined_imgs, filters)</span><br><span class="line">        d2 = _build_discriminator_layer(d1, filters * <span class="number">2</span>)</span><br><span class="line">        d3 = _build_discriminator_layer(d2, filters * <span class="number">4</span>)</span><br><span class="line">        d4 = _build_discriminator_layer(d3, filters * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        x = keras.layers.Flatten()(d4)</span><br><span class="line">        logit = keras.layers.Dense(<span class="number">1</span>)(x)</span><br><span class="line"></span><br><span class="line">        discriminator = keras.models.Model([groundtruth_pianoroll,condition_input], logit,</span><br><span class="line">                                              name=<span class="string">&#x27;Discriminator&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> discriminator</span><br></pre></td></tr></table></figure></li>
<li><p>GAN모델 loss함수와 굴리는 방법</p>
<p><em>Generator Loss 함수 :</em></p>
<ul>
<li><p>Discriminator loss함수와 반대의 함수를 사용한다. 제네레이터는 pianoroll을 가능한 한 더 리얼하게 만들어야 하기때문.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m} -D_w(G(z^{i}|c^{i})|c^{i})$</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generator_loss</span>(<span class="params">discriminator_fake_output</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot; Wasserstein GAN loss</span></span><br><span class="line"><span class="string">(Generator)  -D(G(z|c))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> -tf.reduce_mean(discriminator_fake_output)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><em>Discriminator Loss 함수:</em></p>
<ul>
<li><p>진짜 Pianoroll과 생성된 pianoroll 분포의 거리를 최대화 하기 위해 Wasserstein loss 함수를 사용.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m} [D_w(G(z^{i}|c^{i})|c^{i}) - D_w(x^{i}|c^{i})]$</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wasserstein_loss</span>(<span class="params">discriminator_real_output, discriminator_fake_output</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot; Wasserstein GAN loss</span></span><br><span class="line"><span class="string">(Discriminator)  D(G(z|c)) - D(x|c)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> tf.reduce_mean(discriminator_fake_output) - tf.reduce_mean(</span><br><span class="line">  discriminator_real_output)</span><br></pre></td></tr></table></figure>
</li>
<li><p>gradient penalty loss(aka WGAN-GP loss)를 사용한다.<br> 그 이유는 D에 대한 gradient를 적절히 컨트롤 하는데 적합하기 떄문이고 G의 최적화에 도움을 준다.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m}(\lVert \nabla_{\hat{x}^i}D_w(\hat{x}^i|c^{i}) \rVert_2 -  1)^2 $</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_penalty</span>(<span class="params">discriminator, x, fake_x</span>):</span><br><span class="line"></span><br><span class="line">c = tf.expand_dims(x[..., <span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">batch_size = x.get_shape().as_list()[<span class="number">0</span>]</span><br><span class="line">eps_x = tf.random.uniform(</span><br><span class="line">    [batch_size] + [<span class="number">1</span>] * (<span class="built_in">len</span>(x.get_shape()) - <span class="number">1</span>))  <span class="comment"># B, 1, 1, 1, 1</span></span><br><span class="line">inter = eps_x * x + (<span class="number">1.0</span> - eps_x) * fake_x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> g:</span><br><span class="line">    g.watch(inter)</span><br><span class="line">    disc_inter_output = discriminator((inter,c), training=<span class="literal">True</span>)</span><br><span class="line">grads = g.gradient(disc_inter_output, inter)</span><br><span class="line">slopes = tf.sqrt(<span class="number">1e-8</span> + tf.reduce_sum(</span><br><span class="line">    tf.square(grads),</span><br><span class="line">    axis=tf.<span class="built_in">range</span>(<span class="number">1</span>, grads.get_shape().ndims)))</span><br><span class="line">gradient_penalty = tf.reduce_mean(tf.square(slopes - <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradient_penalty</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="GAN-모델-돌리는-방법"><a href="#GAN-모델-돌리는-방법" class="headerlink" title="GAN 모델 돌리는 방법"></a>GAN 모델 돌리는 방법</h3><ol>
<li><p>G와 D에 Adam 최적화 함수를 쓴다.(정교하게 하려면 SGD 를 써야 한다.)  </p>
</li>
<li><p>체크포인트를 써서 매번 모델을 저장한다.</p>
</li>
<li><p>돌려보는 함수.<br>  G를 돌리는 함수 부분  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator_train_step</span>(<span class="params">x, condition_track_idx=<span class="number">0</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment">############################################</span></span><br><span class="line">    <span class="comment"># G를 업데이트 한다.: maximize D(G(z|c))</span></span><br><span class="line">    <span class="comment">############################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 조건부 트랙을 뽑아서 real batch pianoroll을 만든다.</span></span><br><span class="line"></span><br><span class="line">    c = tf.expand_dims(x[..., condition_track_idx], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># latent vectors의 batch data를 만든다.</span></span><br><span class="line">    z = tf.random.truncated_normal([BATCH_SIZE, <span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        fake_x = generator((c, z), training=<span class="literal">True</span>)</span><br><span class="line">        fake_output = discriminator((fake_x,c), training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># G 결과물의 Loss 계산한다.</span></span><br><span class="line">        gen_loss = generator_loss(fake_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># G의 gradient를 계산한다.</span></span><br><span class="line">    gradients_of_generator = tape.gradient(gen_loss,</span><br><span class="line">                                          generator.trainable_variables)</span><br><span class="line">    <span class="comment"># 제네레이터를 업데이트 한다.</span></span><br><span class="line">    generator_optimizer.apply_gradients(</span><br><span class="line">        <span class="built_in">zip</span>(gradients_of_generator, generator.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure>

<p> D를 돌리는 함수 부분  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator_train_step</span>(<span class="params">x, condition_track_idx=<span class="number">0</span></span>):</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"><span class="comment">#(2) D를 업데이트: (D(x|c)) + (1 - D(G(z|c))|c) + GradientPenality() 를 최대화</span></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 조건부 트랙을 뽑아서 real batch pianoroll을 만든다.</span></span><br><span class="line">c = tf.expand_dims(x[..., condition_track_idx], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># latent vectors의 batch data를 만든다.</span></span><br><span class="line">z = tf.random.truncated_normal([BATCH_SIZE, <span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훼이크 pianoroll을 만든다.</span></span><br><span class="line">fake_x = generator((c, z), training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># D의 파라미터들 업데이트</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    real_output = discriminator((x,c), training=<span class="literal">True</span>)</span><br><span class="line">    fake_output = discriminator((fake_x,c), training=<span class="literal">True</span>)</span><br><span class="line">    discriminator_loss =  wasserstein_loss(real_output, fake_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># real, fake batch의 gradient를 계산</span></span><br><span class="line">grads_of_discriminator = tape.gradient(discriminator_loss,</span><br><span class="line">                                           discriminator.trainable_variables)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    gp_loss = compute_gradient_penalty(discriminator, x, fake_x)</span><br><span class="line">    gp_loss *= <span class="number">10.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># real, fake batch의 GP-loss를 계산</span></span><br><span class="line">grads_gp = tape.gradient(gp_loss, discriminator.trainable_variables)</span><br><span class="line">gradients_of_discriminator = [g + ggp <span class="keyword">for</span> g, ggp <span class="keyword">in</span></span><br><span class="line">                              <span class="built_in">zip</span>(grads_of_discriminator, grads_gp)</span><br><span class="line">                              <span class="keyword">if</span> ggp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># D를 업데이트 해준다.</span></span><br><span class="line">discriminator_optimizer.apply_gradients(</span><br><span class="line">    <span class="built_in">zip</span>(gradients_of_discriminator, discriminator.trainable_variables))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> discriminator_loss + gp_loss</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Multi-GPU-돌리는-방법-소개"><a href="#Multi-GPU-돌리는-방법-소개" class="headerlink" title="Multi-GPU 돌리는 방법 소개"></a>Multi-GPU 돌리는 방법 소개</h3><ol>
<li>텐서플로 MirroredStrategy 를 사용(계산 용이성)</li>
<li>어떻게 돌리는지?? <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Strategy 만들고</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()</span><br><span class="line">FLAG = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> strategy.num_replicas_in_sync  &gt; <span class="number">1</span> <span class="keyword">and</span> FLAG:</span><br><span class="line">    MULTIPLE_BATCH = strategy.num_replicas_in_sync</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;분산환경 사용 &gt;&gt; GPU: <span class="subst">&#123;MULTIPLE_BATCH&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;분산환경 미사용&#x27;</span>)</span><br><span class="line">  MULTIPLE_BATCH = <span class="number">1</span></span><br><span class="line"><span class="comment"># 2. 모델을 strategy 안에 포함시킴.</span></span><br><span class="line"><span class="comment"># 중요한 부분 : 학습 함수에서 나온 loss를 self.strategy.run에 넣고 strategy.reduce</span></span><br><span class="line"><span class="comment"># 배치 사이즈를 num_replica_in_sync 갯수만큼 곱한다.</span></span><br><span class="line"><span class="comment"># 출력을 포함한 자세한 코드는 My GitHub...</span></span><br></pre></td></tr></table></figure>
 <a target="_blank" rel="noopener" href="https://github.com/ikarus-999/MuseGAN_TF2.x/blob/main/museGAN_V3_tf2.ipynb">https://github.com/ikarus-999/MuseGAN_TF2.x/blob/main/museGAN_V3_tf2.ipynb</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-07-15T10:43:57.000Z" title="2021. 7. 15. 오후 7:43:57">2021-07-15</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:19:30.445Z" title="2022. 4. 5. 오후 3:19:30">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">6분안에 읽기 (약 960 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a></h1><div class="content"><h2 id="딥러닝-논문-리뷰"><a href="#딥러닝-논문-리뷰" class="headerlink" title="딥러닝 논문 리뷰"></a>딥러닝 논문 리뷰</h2><p>  RNN의 고질적인 문제점… Sequence가 길면<br>  i번째 output을 만들기 위해 그 이전의 i-1번째 hidden state를 사용한다.<br>  Long Term Dependency problem…<br>  <img src="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%ED%8F%AC%EB%A8%B8_01.png" alt="이런거"><br>  <img src="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%ED%8F%AC%EB%A8%B8_pre.png" alt="Gradient X"><br>  이걸 해결하기 위해서는 Recurrent Layer 대신<br>  Attention Mechanism을 쓰면 Sequence 길이에 상관없이 input &#x2F; output의 Dependency를 보다  정확히 감지… RNN을 완전히 제거해야 함.  </p>
<h3 id="트랜스포머를-써야-하는-이유"><a href="#트랜스포머를-써야-하는-이유" class="headerlink" title="트랜스포머를 써야 하는 이유?"></a>트랜스포머를 써야 하는 이유?</h3><blockquote>
<p>현재 Video Understanding 모델에서도 시도되고 있음(TimesFormer; 기존 프레임 단위로 쪼갠 CNN모델에 비해서 메모리 절감과 Inference 속도 향상)<br>  Timesformer 은 효과적인 프레임별 델타값만 감지 : 비디오를 Patch단위로 분석함    </p>
</blockquote>
<ul>
<li>Positional Encoding을 시간축에 확장한 모델, 시간축과 공간축 전체를 어텐션 하면 Cost가 너무 크기 때문<br>  밀리초 단위로 결판이 나는 게임 판독에도 정말 용이할것으로 기대됨.</li>
</ul>
<h3 id="트랜스포머의-특징"><a href="#트랜스포머의-특징" class="headerlink" title="트랜스포머의 특징"></a>트랜스포머의 특징</h3><ul>
<li><p>RNN 계열은 순서대로만 처리 가능해서 학습 속도가 느림.<br>  하지만 트랜스포머는 병렬 처리가 가능…How?  </p>
</li>
<li><p>Encoder-Decoder 모델을 통해서 병렬 처리  </p>
<blockquote>
<p>ENcoder에서는 각각 position에 대해 Attention만 하고,<br>  Decoder에서는 Masking 메커니즘으로 병렬 처리가 가능</p>
</blockquote>
</li>
</ul>
<p>  Encoder는 input Sequence를 다른 표현으로 치환  </p>
<ul>
<li>Decoder에서는 ENcoder으로부터 Output Sequence를 하나씩 생성  </li>
<li>각각 step에서 다음 symbol을 만들 때 이전에 만들어진 output을 쓴다.(자기 회귀적인 특성)  <blockquote>
<p>ex : “여기는 어디 나는 누구” 라는 문장에서 “여기는 어디” 라는 symbol으로 “나는 누구” 를 만들 수 있습니다.</p>
</blockquote>
</li>
</ul>
<h3 id="Transformer-전체-구조"><a href="#Transformer-전체-구조" class="headerlink" title="Transformer 전체 구조"></a>Transformer 전체 구조</h3><p>  <img src="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/transformer.png" alt="이것이 바로 트랜스포머">   </p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>  <img src="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_%EC%9D%B8%EC%BD%94%EB%8D%94.png" alt="인코더"></p>
<ul>
<li><p>Input Embedding은 Time Embedding, 자연어에 쓰이는 Word Embedding 등 여러 종류가 있음  </p>
<p>그 중에서 Time Embedding을 소개.<br>$$\mathbf{t} 2 \mathbf{v}(\tau)[i]&#x3D;\left{\begin{array}{ll}<br>\omega_{i} \tau+\varphi_{i}, &amp; \text { if } i&#x3D;0 \<br>\mathcal{F}\left(\omega_{i} \tau+\varphi_{i}\right), &amp; \text { if } 1 \leq i \leq k<br>\end{array}\right.$$</p>
<p>ax+b처럼 생긴 저 식에 함수를 넣어서 시간별 정보를 실어야 함.<br>i는 Timestep…  시퀀스의 시작점은 그냥 ax+b만을 쓴다.<br>전체 시퀀스 데이터에 적용하려면 주기성을 갖는 함수(파장, 주기, 주파수 등)를 사용해야 하는데<br>relu를 사용하면 주기성 정보가 없으니…당연히 안될듯..(!) </p>
<p>y &#x3D; wx + b concat sin(wx+b)</p>
<p>각 타임스텝별로 주기성 정보를 주입  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.Keras.Layer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Time2Vector</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_len, **kwargs</span>):</span><br><span class="line">      <span class="built_in">super</span>(Time2Vector, self).__init__()</span><br><span class="line">      self.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;shape (batch, seq_len) 형태로 가중치와 Bias 초기화 &#x27;&#x27;&#x27;</span></span><br><span class="line">      self.weights_linear = self.add_weight(name=<span class="string">&#x27;weight_linear&#x27;</span>,shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_linear = self.add_weight(name=<span class="string">&#x27;bias_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.weights_periodic = self.add_weight(name=<span class="string">&#x27;weight_periodic&#x27;</span>,shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_periodic = self.add_weight(name=<span class="string">&#x27;bias_periodic&#x27;</span>,shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;주기성, 선형 시간별 특징을 계산&#x27;&#x27;&#x27;</span></span><br><span class="line">      x = tf.math.reduce_mean(x[:,:,:], axis=-<span class="number">1</span>) <span class="comment"># 입력 Feature 차원 슬라이싱</span></span><br><span class="line">      time_linear = self.weights_linear * x + self.bias_linear <span class="comment"># 선형 시간 특징</span></span><br><span class="line">      time_linear = tf.expand_dims(time_linear, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line"></span><br><span class="line">      time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)</span><br><span class="line">      time_periodic = tf.expand_dims(time_periodic, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line">      <span class="keyword">return</span> tf.concat([time_linear, time_periodic], axis=-<span class="number">1</span>) <span class="comment"># shape = (batch, seq_len, 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p>positional Encoding<br>$$\begin{array}{l}<br>P E_{(\text {pos } 2 i)}&#x3D;\sin \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_{\text {model }}}}}\right) \<br>P E_{(\text {pos 2i+1) }}&#x3D;\cos \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_<br>{\text {model }}}}} \right)<br>\end{array}$$</p>
<p>짝수 번째에는 sin, 홀수 번쨰에는 cos함수…</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-02-01T12:49:29.000Z" title="2021. 2. 1. 오후 9:49:29">2021-02-01</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:23:13.521Z" title="2022. 4. 5. 오후 3:23:13">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">2분안에 읽기 (약 256 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/02/01/%EC%BD%94%EB%94%A9-%EA%B5%AD%EB%A3%B0/">코딩 국룰</a></h1><div class="content"><h2 id="코딩-국룰"><a href="#코딩-국룰" class="headerlink" title="코딩 국룰"></a>코딩 국룰</h2><p>  C++, C#, java에서는 getter, setter<br>  Python에서는 변수를 더블 언더스코어(__)로 접근함.</p>
<h3 id="Getter-Setter"><a href="#Getter-Setter" class="headerlink" title="Getter, Setter?"></a>Getter, Setter?</h3><p>  Getter : 해당 함수를 통해서 값을 얻는 함수<br>  Setter : 해당 함수를 통해서 값을 설정하는 함수</p>
<p>  만약 모든 변수들을 public으로 한다면??  </p>
<ul>
<li><p>특히 파이썬에서는 자주 있는 일이다.<br>   정말 대환장 페스티벌이다. 어디서 꼬였는지 찾기도 힘들다.<br>   다른 함수에서 주요 변수를 조작하기 때문이다.  </p>
</li>
<li><p>쉽게 말하면 은행에 있는 돈은 인가자만 취급가능하다.<br>   입출금도 정해진 방법(함수)로만 핸들링 되어야 한다.<br>   은행에서 돈을 인출할 때는 Getter로 꺼내고 Setter로 돈을 맡기는 원리이다.<br>   private 변수를 함수를 통해 조작해야 할 일이 있는데 그것이 Getter, Setter이다.</p>
</li>
</ul>
<ol>
<li><p>Getter 예제 (C#)</p>
  <figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> Level;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="built_in">int</span> <span class="title">GetLevel</span>()</span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Level;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Setter 예제</p>
  <figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">SetLevel</span>(<span class="params"><span class="built_in">int</span> Level</span>)</span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.Level = Level</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-30T06:14:16.000Z" title="2021. 1. 30. 오후 3:14:16">2021-01-30</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:21:31.797Z" title="2022. 4. 5. 오후 3:21:31">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Dacon/">Dacon</a></span><span class="level-item">2분안에 읽기 (약 366 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/30/Dacon%ED%9B%84%EA%B8%B0/">Dacon후기</a></h1><div class="content"><h2 id="저번에-이은-데이콘-도전-후기2"><a href="#저번에-이은-데이콘-도전-후기2" class="headerlink" title="저번에 이은 데이콘 도전 후기2"></a>저번에 이은 데이콘 도전 후기2</h2><p>  순위 발표 순간…<br>  ?!?!?!?!<br>  <img src="/2021/01/30/Dacon%ED%9B%84%EA%B8%B0/dacon01.PNG" alt="순위 발표"></p>
<p>  <img src="/2021/01/30/Dacon%ED%9B%84%EA%B8%B0/dacon02.PNG" alt="자세한 순위"></p>
<p>  1458 팀 중 15위… 와 이거 실화?!</p>
<p>  모델링은 Conv-LSTM</p>
<p>  전처리 방법은 설명력 높은 변수, </p>
<p>  단위면적당 일사량 &#x3D; 산란일사량 + 직접 일사량</p>
<p>  GHI &#x3D; DHI + DNI * \(\cos(\theta))\)  </p>
<p>  하루의 전체 시간 중에서</p>
<p>  해가 뜨고 질 때까지만 계산하는 것이였다.  </p>
<p>  이것이 바로 그 방법들…</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GHI 계산 방법</span></span><br><span class="line">temp[<span class="string">&#x27;NoSun&#x27;</span>] = np.where((temp[<span class="string">&#x27;DHI&#x27;</span>] &gt; <span class="number">0</span>) | (temp[<span class="string">&#x27;DNI&#x27;</span>] &gt; <span class="number">0</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">temp[<span class="string">&#x27;sunny&#x27;</span>] = temp.groupby([<span class="string">&#x27;Day&#x27;</span>, temp.NoSun.cumsum()])[<span class="string">&#x27;NoSun&#x27;</span>].apply(<span class="keyword">lambda</span> x: (x ^ <span class="number">1</span>).cumsum())</span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;long&#x27;</span>] = temp[<span class="string">&#x27;sunny&#x27;</span>].cummax()</span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;angle&#x27;</span>] = ((temp[<span class="string">&#x27;sunny&#x27;</span>] / temp[<span class="string">&#x27;long&#x27;</span>]) * <span class="number">180</span>) - <span class="number">90</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;GHI&#x27;</span>] = temp[<span class="string">&#x27;DHI&#x27;</span>] + temp[<span class="string">&#x27;DNI&#x27;</span>] * temp[<span class="string">&#x27;angle&#x27;</span>].apply(<span class="keyword">lambda</span> x: np.cos(np.pi * (x / <span class="number">180</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataSet windowing 하는 방법</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">windowed_dataset</span>(<span class="params">x, y, window_size, batch_size, shuffle, shuffle_size</span>):</span><br><span class="line">  ds_x = tf.data.Dataset.from_tensor_slices(x)</span><br><span class="line">  ds_x = ds_x.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">  ds_x = ds_x.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">  </span><br><span class="line">  ds_y = tf.data.Dataset.from_tensor_slices(y)</span><br><span class="line">  ds_y = ds_y.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">  ds_y = ds_y.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">  </span><br><span class="line">  ds = tf.data.Dataset.<span class="built_in">zip</span>((ds_x, ds_y))</span><br><span class="line">  <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(shuffle_size)</span><br><span class="line">  <span class="keyword">return</span> ds.batch(batch_size).prefetch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_data_pred</span>(<span class="params">data, window_size, batch_size</span>):</span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices(data)</span><br><span class="line">    ds = ds.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>) <span class="comment">#, drop_remainder=True) # stride = 1</span></span><br><span class="line">    ds = ds.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds.padded_batch(batch_size, padded_shapes=(<span class="literal">None</span>, <span class="number">7</span>)).prefetch(<span class="number">1</span>) <span class="comment"># 7은 Test Set의 특징 개수</span></span><br></pre></td></tr></table></figure>
<h3 id="Test-dataset을-drop-remainder-x3D-True-하면-안되는-이유"><a href="#Test-dataset을-drop-remainder-x3D-True-하면-안되는-이유" class="headerlink" title="Test dataset을 drop_remainder&#x3D;True 하면 안되는 이유?"></a>Test dataset을 drop_remainder&#x3D;True 하면 안되는 이유?</h3><p>  Test Set의 정보가 sequence length만큼 없어짐.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-03T04:05:29.000Z" title="2021. 1. 3. 오후 1:05:29">2021-01-03</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:20:21.669Z" title="2022. 4. 5. 오후 3:20:21">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5분안에 읽기 (약 677 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/">소소한 연구아닌 연구</a></h1><div class="content"><h2 id="넘사벽-난이도인-트랜스포머-모델링"><a href="#넘사벽-난이도인-트랜스포머-모델링" class="headerlink" title="넘사벽 난이도인 트랜스포머 모델링"></a>넘사벽 난이도인 트랜스포머 모델링</h2><p>  시계열 데이터를 병렬 처리한다는 점에서는 가장 좋지만<br>  그만큼 모델링 하기가 까다롭다.</p>
<p>  model input : (Batch, Sequence, feature_num)</p>
<p>  Transformer는 input &#x2F; output을 잘못 설계하다간 sequence 정보가 날아갈 수도 있고<br>  그냥 데이터 자체가 8:45 가 될수가 있다.</p>
<p>  입력 데이터를 Seqneuce로 Packing 해주어야 학습이 가능한 데이터가 된다.</p>
<p>  Sequence Packing 방법은 파이썬 문법으로 비교적 쉽게 되지만…</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_train = train.values</span><br><span class="line">train_seq = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len, <span class="built_in">len</span>(df_train)+<span class="number">1</span>):</span><br><span class="line">  train_seq.append(df_train[i - seq_len: i])</span><br><span class="line">train_seq = np.array(train_seq)</span><br></pre></td></tr></table></figure>

<p>  Sequence unpacking 은 numpy 연산을 써야 비교적 쉬움..<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = np.vstack(r <span class="keyword">for</span> r <span class="keyword">in</span> train_seq)</span><br><span class="line">out = out.unique(out, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>  <img src="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A41.PNG" alt="packing &amp; unpacking"></p>
<p>  <img src="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A42.PNG" alt="unpacking2"></p>
<h3 id="다변량-회귀에서-얻은-교훈-1"><a href="#다변량-회귀에서-얻은-교훈-1" class="headerlink" title="다변량 회귀에서 얻은 교훈 1"></a>다변량 회귀에서 얻은 교훈 1</h3><p>  입력 데이터를 받아서 Time Embedding…</p>
<p>  시간별 Positional Encoding 방법을 소개하지.</p>
<p>  y &#x3D; wx + b concat sin(wx+b)</p>
<p>  선형의 시간 특징과 주기성의 특징에다 sin을 적용한 특징임.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Time2Vector</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_len, **kwargs</span>):</span><br><span class="line">      <span class="built_in">super</span>(Time2Vector, self).__init__()</span><br><span class="line">      self.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;shape (batch, seq_len) 형태로 가중치와 Bias 초기화 &#x27;&#x27;&#x27;</span></span><br><span class="line">      self.weights_linear = self.add_weight(name=<span class="string">&#x27;weight_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_linear = self.add_weight(name=<span class="string">&#x27;bias_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.weights_periodic = self.add_weight(name=<span class="string">&#x27;weight_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_periodic = self.add_weight(name=<span class="string">&#x27;bias_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;주기성, 선형 시간별 특징을 계산&#x27;&#x27;&#x27;</span></span><br><span class="line">      x = tf.math.reduce_mean(x[:,:,:], axis=-<span class="number">1</span>) <span class="comment"># 입력 Feature 차원 슬라이싱</span></span><br><span class="line">      time_linear = self.weights_linear * x + self.bias_linear <span class="comment"># 선형 시간 특징</span></span><br><span class="line">      time_linear = tf.expand_dims(time_linear, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line"></span><br><span class="line">      time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)</span><br><span class="line">      time_periodic = tf.expand_dims(time_periodic, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line">      <span class="keyword">return</span> tf.concat([time_linear, time_periodic], axis=-<span class="number">1</span>) <span class="comment"># shape = (batch, seq_len, 2)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="트랜스포머의-특징"><a href="#트랜스포머의-특징" class="headerlink" title="트랜스포머의 특징"></a>트랜스포머의 특징</h3><p>  시계열 데이터 분류, 회귀 문제에서는<br>  Decoder가 빠져있는 Self Attention을 사용한다.  (Fine Tunning)</p>
<p>  Decoder는 챗봇이나 Auto-Encoder같은 Encoder-Decoder 구조에 사용되는 경향이 있다.</p>
<p>  참고 논문 : <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=lE1AB4stmX">https://openreview.net/forum?id=lE1AB4stmX</a></p>
<p>  논문에서는 n_layer_num, padding_mask 를 쓰는 특징이 있다. 이 파라미터까지 쓸 수 있으면 좋겠지만<br>  GPU 메모리가 8기가밖에 안되서 논문 파라미터보다는 작게 설정을 해야 원활이 가능하다.<br>  그리고 dataset을 window 하면 GPU RAM 사용량을 더 낮출 수 있다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-02T12:56:29.000Z" title="2021. 1. 2. 오후 9:56:29">2021-01-02</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:21:13.794Z" title="2022. 4. 5. 오후 3:21:13">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Kaggle-Dacon/">Kaggle, Dacon</a></span><span class="level-item">4분안에 읽기 (약 557 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/">치열한-데이콘후기</a></h1><div class="content"><h2 id="데이콘-최근-대회-후기-데이콘-어나더-버전"><a href="#데이콘-최근-대회-후기-데이콘-어나더-버전" class="headerlink" title="데이콘 최근 대회 후기 - 데이콘 어나더 버전"></a>데이콘 최근 대회 후기 - 데이콘 어나더 버전</h2><p>  흥미롭지만 quantile loss… 정말 만만치 않다.<br>  loss function 수식구현은 쉽지만 loss를 줄이는 방법이 관건이다.<br>  진짜 4.1에서 줄어들지 않는 loss… 과연 어떻게 하면 줄일수 있을까…</p>
<h3 id="다변량-회귀-왜-이렇게-어려울까요-데이콘-대회에서-얻은-엄청난-교훈"><a href="#다변량-회귀-왜-이렇게-어려울까요-데이콘-대회에서-얻은-엄청난-교훈" class="headerlink" title="다변량 회귀 왜 이렇게 어려울까요? - 데이콘 대회에서 얻은 엄청난 교훈"></a>다변량 회귀 왜 이렇게 어려울까요? - 데이콘 대회에서 얻은 엄청난 교훈</h3><p>  loss 계산할때 다른 target 값과 x_train값이 아닌   </p>
<p>  다른 Element 끼리 계산되어서 오히려 loss가 폭증하기도 한다.</p>
<p>  Wandb로 시각화 안했으면 큰일났었을듯 싶었다. </p>
<p>  Loss 시각화 툴로 보니 초반에서 수렴하지 않는 문제나 아예 u자로 불규칙적으로 튀는 현상도 발생….</p>
<p>  <img src="/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/%EC%8B%A0%EA%B8%B0%ED%95%9C%EA%B7%B8%EB%9E%98%ED%94%84.PNG" alt="loss가 줄어드는 것처럼 보이지만 실제는 ..."></p>
<p>  분명 window dataset으로 부하분산을 하면 데이터 처리가 수월할 것 같지만…</p>
<p>  채점을 해보니 극악의 Loss가 나오기도 한다.</p>
<p>  이 떄에는 window_size, buffer size를 잘 조절해야 한다.</p>
<p>  window_size를 너무 낮추면 자칫하면 GPU 사용률은 한자리에 머물고 시간은 오래 걸린다. </p>
<p>  하지만 GPU RAM 사용량은 줄어드는 이점은 있다.</p>
<p>  window dataset을 하다보면 배치 크기가 안 맞아서 예측이 안될때가 있는데 이럴땐…<br>  <img src="/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/%ED%9D%90%EC%9D%B5.PNG" alt="배치 크기가 안맞을 떄는 이렇게 padding을 쓴다."></p>
<p>  Shuffle Buffer size 조절은 필수, batch Size에 주의하여 조절한다.<br>  그렇지 않으면(BATCH SIZE가 굉장히 높다면)<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f2570221d30&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details. </span><br></pre></td></tr></table></figure></p>
<p>  이런 경고가 나온다.. -&gt; batch size를 반드시 조절하거나 batch padding…</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-22T15:13:20.000Z" title="2020. 12. 23. 오전 12:13:20">2020-12-23</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:20:29.787Z" title="2022. 4. 5. 오후 3:20:29">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Kaggle-Dacon/">Kaggle, Dacon</a></span><span class="level-item">3분안에 읽기 (약 453 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/">어나더Dacon도전기_2</a></h1><div class="content"><h2 id="데이콘-또다른-대회-후기-데이콘-어나더-버전🌠🌠"><a href="#데이콘-또다른-대회-후기-데이콘-어나더-버전🌠🌠" class="headerlink" title="데이콘 또다른 대회 후기 - 데이콘 어나더 버전🌠🌠"></a>데이콘 또다른 대회 후기 - 데이콘 어나더 버전🌠🌠</h2><p>  역시 흥미로운 데이터 세계 🐱<br>  Regression, 특히 CNN, LSTM 모델은 Epoch를 늘리면 더욱 정확도가 향상되는 특징이 있다!!  </p>
<p>  너무 많이 늘리면 Overfitting이 날 수 있으니 LR 스케줄러 등 다양한 방법을 시도해봐야 한다.</p>
<h3 id="CNN-LSTM-다변량-회귀-모델링-후기🌠"><a href="#CNN-LSTM-다변량-회귀-모델링-후기🌠" class="headerlink" title="CNN-LSTM 다변량 회귀 모델링 후기🌠"></a>CNN-LSTM 다변량 회귀 모델링 후기🌠</h3><p>  처음에는 왜 이렇게 loss가 줄지 않는지 🎚 의문이 들었다.🌠</p>
<p>  <img src="/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/Multi_var_reg.PNG" alt="loss가 전혀 줄지 않음"><br>  헐… 4.5 라니;;;🔌 20Epoch 가지고는 아예 학습이 안되나보다.</p>
<p>  그러면 Epoch🕜를 늘리는데 그냥 늘리면 안된다💽</p>
<p>  자칫 잘못하면 오버피팅이 발생할 수 있기 때문이다.🕜</p>
<p>  그리고 시계열 문제는 시간, 구간당 평균이나 여러 통계적 변수를 만들어야 점수가 더 잘나오는 특징이 있다.⏩</p>
<p>  <a target="_blank" rel="noopener" href="https://github.com/sachinruk/KerasQuantileModel/blob/master/Keras%20Quantile%20Model.ipynb">참고문서1</a></p>
<p>  역시나 예측이 맞았다. Epoch을 약간(?) 겁나게 늘리고 optimizer나 다른 학습율 스케줄러, sgd에 decay, momentum 을 주어야 하나보다.</p>
<p>  그리고 다변량 회귀이기 때문에 validation loss 계산이 약간 신중해야 정확한 loss값이 계산이 가능하다.</p>
<p>  test dataset에 element-wise 방식으로 계산되는거라 test dataset shape 그대로 predict가 출력된다.</p>
<p>  Reshape, Squeeze 를 써서 shape을 맞춰주어야 한다.</p>
<p>  Quantile Regression 문제는 그냥 Regression에 Quantile별로 값을 추출해야 하는 약간의(?) 난제가 존재한다.</p>
<p>  Pytorch는 Pythonic🐍 하지만 어떤 면에서는 TF2.x 보다 더 까다로운 것 같다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-06T12:48:20.000Z" title="2020. 12. 6. 오후 9:48:20">2020-12-06</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:21:25.155Z" title="2022. 4. 5. 오후 3:21:25">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Kaggle-Dacon/">Kaggle, Dacon</a></span><span class="level-item">2분안에 읽기 (약 331 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/">Dacon도전기</a></h1><div class="content"><h2 id="데이콘-도전-후기"><a href="#데이콘-도전-후기" class="headerlink" title="데이콘 도전 후기"></a>데이콘 도전 후기</h2><p>  흥미롭지만 쉽지는 않다.<br>  말 그대로 새로운 파생변수, 모델을 많이 만들면 점수가 오르긴 한다.</p>
<p>  <a target="_blank" rel="noopener" href="https://dacon.io/competitions/open/235597/leaderboard/">데이콘_연습문제</a></p>
<p>  연습용이긴 한데 BERT모델 폭격으로 <del>양민학살</del> 이 벌써부터 시작되었…ㅎㄷㄷ</p>
<p>  버트모델 안쓰고도 순위 올릴수는 있다. </p>
<p>  지금부터 그 방법을 소개합니다.</p>
<h3 id="데이콘-순위를-올릴수-있는-방법-창의적인-modeling-K-cross-Validation-parameter-searching"><a href="#데이콘-순위를-올릴수-있는-방법-창의적인-modeling-K-cross-Validation-parameter-searching" class="headerlink" title="데이콘 순위를 올릴수 있는 방법 - 창의적인 modeling, K-cross Validation, parameter searching"></a>데이콘 순위를 올릴수 있는 방법 - 창의적인 modeling, K-cross Validation, parameter searching</h3><p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_1.PNG" alt="이것이 바로 VDCNN">  </p>
<p>  VDCNN은 GPU가 잘 버텨주면<br>  0.86까지는 마구마구 올릴수 있음.  </p>
<p>  이것이 바로 VDCNN이당!<br>  <a target="_blank" rel="noopener" href="https://github.com/ikarus-999/VDCNN_kor"><img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_5.PNG" alt="VDCNN code"></a></p>
<p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_9.PNG" alt="VDCNN 학습장면"></p>
<p>  학습 모델을 그림으로 보기</p>
<p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_36_0.png" alt="VDCNN 그림1"></p>
<p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/Dacon01.PNG" alt="데이콘 결과"><br>  여기에 여러 가지 모델 Ensemble 해보면 0.88은 가능해보일 것 같네요.</p>
<blockquote>
<p>추가로 모델 Ensemble 을 시도<br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_43_0.png" alt="추가했던 모델1"><br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_50_0.png" alt="추가했던 모델2"><br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_57_0.png" alt="추가했던 모델3"></p>
</blockquote>
<blockquote>
<p>추가적으로 트랜스포머 모델링..(2020-12-09 추가…)<br>  <a target="_blank" rel="noopener" href="https://github.com/ikarus-999/VDCNN_kor/blob/master/TF2_kor_2.0.ipynb">Transformer</a></p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-06T06:56:21.000Z" title="2020. 12. 6. 오후 3:56:21">2020-12-06</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-05T06:21:57.533Z" title="2022. 4. 5. 오후 3:21:57">2022-04-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Kaggle-Dacon/">Kaggle, Dacon</a></span><span class="level-item">17분안에 읽기 (약 2594 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/06/kaggle-try/">kaggle_try</a></h1><div class="content"><h2 id="남들이-잘-안하는-데이터셋으로-캐글-도전-후기"><a href="#남들이-잘-안하는-데이터셋으로-캐글-도전-후기" class="headerlink" title="남들이 잘 안하는 데이터셋으로 캐글 도전 후기"></a>남들이 잘 안하는 데이터셋으로 캐글 도전 후기</h2><p>  데이터셋 링크 :<br>  <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/LANL-Earthquake-Prediction">https://www.kaggle.com/c/LANL-Earthquake-Prediction</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.initializers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> random, sys</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;precision&#x27;</span>, <span class="number">30</span>)</span><br><span class="line">np.set_printoptions(precision = <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">47</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train.csv&#x27;</span>, dtype=&#123;<span class="string">&#x27;acoustic_data&#x27;</span>: np.int8, <span class="string">&#x27;time_to_failure&#x27;</span>: np.float32&#125;)</span><br></pre></td></tr></table></figure>

<pre><code>CPU times: user 1min 30s, sys: 19.1 s, total: 1min 49s
Wall time: 1min 49s
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = train_df[<span class="string">&#x27;acoustic_data&#x27;</span>].values</span><br><span class="line">y_train = train_df[<span class="string">&#x27;time_to_failure&#x27;</span>].values</span><br></pre></td></tr></table></figure>

<h3 id="Training-data에서-완전한-데이터-구간-찾기-0에-근접한-failure-time-찾기"><a href="#Training-data에서-완전한-데이터-구간-찾기-0에-근접한-failure-time-찾기" class="headerlink" title="Training data에서 완전한 데이터 구간 찾기(0에 근접한  failure time 찾기)"></a>Training data에서 완전한 데이터 구간 찾기(0에 근접한  failure time 찾기)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ends_mask = np.less(y_train[:-<span class="number">1</span>], y_train[<span class="number">1</span>:])</span><br><span class="line">segment_ends = np.nonzero(ends_mask)</span><br><span class="line"></span><br><span class="line">train_segments = []</span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> end <span class="keyword">in</span> segment_ends[<span class="number">0</span>]:</span><br><span class="line">    train_segments.append((start, end))</span><br><span class="line">    start = end</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(train_segments)</span><br></pre></td></tr></table></figure>

<pre><code>[(0, 5656573), (5656573, 50085877), (50085877, 104677355), (104677355, 138772452), (138772452, 187641819), (187641819, 218652629), (218652629, 245829584), (245829584, 307838916), (307838916, 338276286), (338276286, 375377847), (375377847, 419368879), (419368879, 461811622), (461811622, 495800224), (495800224, 528777114), (528777114, 585568143), (585568143, 621985672)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.set_title(<span class="string">&#x27;segment size&#x27;</span>)</span><br><span class="line">ax.bar(np.arange(<span class="built_in">len</span>(train_segments)), [ s[<span class="number">1</span>] - s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> train_segments])</span><br></pre></td></tr></table></figure>




<pre><code>&lt;BarContainer object of 16 artists&gt;
</code></pre>
<p><img src="/2020/12/06/kaggle-try/output_6_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EarthQuakeRandom</span>(tf.keras.utils.<span class="type">Sequence</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y, x_mean, x_std, segments, ts_length, batch_size, steps_per_epoch</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        self.segments = segments</span><br><span class="line">        self.ts_length = ts_length</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.steps_per_epoch = steps_per_epoch</span><br><span class="line">        self.segments_size = np.array([s[<span class="number">1</span>] - s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> segments])</span><br><span class="line">        self.segments_p = self.segments_size / self.segments_size.<span class="built_in">sum</span>()</span><br><span class="line">        self.x_mean = x_mean</span><br><span class="line">        self.x_std = x_std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.batch_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_ts_length</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ts_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_segments</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.segments</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_segments_p</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.segments_p</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_segments_size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.segments_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.steps_per_epoch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        segment_index = np.random.choice(<span class="built_in">range</span>(<span class="built_in">len</span>(self.segments)), p=self.segments_p)</span><br><span class="line">        segment = self.segments[segment_index]</span><br><span class="line">        end_indexes = np.random.randint(segment[<span class="number">0</span>] + self.ts_length, segment[<span class="number">1</span>], size=self.batch_size)</span><br><span class="line"></span><br><span class="line">        x_batch = np.empty((self.batch_size, self.ts_length))</span><br><span class="line">        y_batch = np.empty(self.batch_size, )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, end <span class="keyword">in</span> <span class="built_in">enumerate</span>(end_indexes):</span><br><span class="line">            x_batch[i, :] = self.x[end - self.ts_length: end]</span><br><span class="line">            y_batch[i] = self.y[end - <span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#x_batch = (x_batch - self.x_mean)/self.x_std</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.expand_dims(x_batch, axis=<span class="number">2</span>), y_batch</span><br></pre></td></tr></table></figure>

<h3 id="train-x2F-validation-나누기-segments"><a href="#train-x2F-validation-나누기-segments" class="headerlink" title="train &#x2F; validation 나누기(segments)"></a>train &#x2F; validation 나누기(segments)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t_segments = [train_segments[i] <span class="keyword">for</span> i <span class="keyword">in</span> [ <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]</span><br><span class="line">v_segments = [train_segments[i] <span class="keyword">for</span> i <span class="keyword">in</span> [ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="training-data에-대해서만-평균과-표준편차-계산"><a href="#training-data에-대해서만-평균과-표준편차-계산" class="headerlink" title="training data에 대해서만 평균과 표준편차 계산"></a>training data에 대해서만 평균과 표준편차 계산</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x_sum = <span class="number">0.</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> t_segments:</span><br><span class="line">    x_sum += X_train[s[<span class="number">0</span>]:s[<span class="number">1</span>]].<span class="built_in">sum</span>()</span><br><span class="line">    count += (s[<span class="number">1</span>] - s[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">X_train_mean = x_sum/count</span><br><span class="line"></span><br><span class="line">x2_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> t_segments:</span><br><span class="line">    x2_sum += np.power(X_train[s[<span class="number">0</span>]:s[<span class="number">1</span>]] - X_train_mean, <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">X_train_std =  np.sqrt(x2_sum/count)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_train_mean, X_train_std)</span><br></pre></td></tr></table></figure>

<pre><code>4.472289301190891 6.189013535612676
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_gen = EarthQuakeRandom(</span><br><span class="line">    x = X_train, </span><br><span class="line">    y = y_train,</span><br><span class="line">    x_mean = X_train_mean, </span><br><span class="line">    x_std = X_train_std,</span><br><span class="line">    segments = t_segments,</span><br><span class="line">    ts_length = <span class="number">150000</span>,</span><br><span class="line">    batch_size = <span class="number">64</span>,</span><br><span class="line">    steps_per_epoch = <span class="number">400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">valid_gen = EarthQuakeRandom(</span><br><span class="line">    x = X_train, </span><br><span class="line">    y = y_train,</span><br><span class="line">    x_mean = X_train_mean, </span><br><span class="line">    x_std = X_train_std,</span><br><span class="line">    segments = v_segments,</span><br><span class="line">    ts_length = <span class="number">150000</span>,</span><br><span class="line">    batch_size = <span class="number">64</span>,</span><br><span class="line">    steps_per_epoch = <span class="number">400</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/shujian/transformer-with-lstm</span></span><br><span class="line"></span><br><span class="line">embed_size = <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps=<span class="number">1e-6</span>, **kwargs</span>):</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).__init__(**kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        self.gamma = self.add_weight(name=<span class="string">&#x27;gamma&#x27;</span>, shape=input_shape[-<span class="number">1</span>:],</span><br><span class="line">                                     initializer=Ones(), trainable=<span class="literal">True</span>)</span><br><span class="line">        self.beta = self.add_weight(name=<span class="string">&#x27;beta&#x27;</span>, shape=input_shape[-<span class="number">1</span>:],</span><br><span class="line">                                    initializer=Zeros(), trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).build(input_shape)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = K.mean(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        std = K.std(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_output_shape</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="keyword">return</span> input_shape</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, attn_dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        self.temper = np.sqrt(d_model)</span><br><span class="line">        self.dropout = Dropout(attn_dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, q, k, v, mask</span>):</span><br><span class="line">        attn = Lambda(<span class="keyword">lambda</span> x:K.batch_dot(x[<span class="number">0</span>],x[<span class="number">1</span>],axes=[<span class="number">2</span>,<span class="number">2</span>])/self.temper)([q, k])</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mmask = Lambda(<span class="keyword">lambda</span> x:(-<span class="number">1e+10</span>)*(<span class="number">1</span>-x))(mask)</span><br><span class="line">            attn = Add()([attn, mmask])</span><br><span class="line">        attn = Activation(<span class="string">&#x27;softmax&#x27;</span>)(attn)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line">        output = Lambda(<span class="keyword">lambda</span> x:K.batch_dot(x[<span class="number">0</span>], x[<span class="number">1</span>]))([attn, v])</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>():</span><br><span class="line">    <span class="comment"># mode 0 - big martixes, faster; mode 1 - more clear implementation</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout, mode=<span class="number">0</span>, use_norm=<span class="literal">True</span></span>):</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="number">0</span>:</span><br><span class="line">            self.qs_layer = Dense(n_head*d_k, use_bias=<span class="literal">False</span>)</span><br><span class="line">            self.ks_layer = Dense(n_head*d_k, use_bias=<span class="literal">False</span>)</span><br><span class="line">            self.vs_layer = Dense(n_head*d_v, use_bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="number">1</span>:</span><br><span class="line">            self.qs_layers = []</span><br><span class="line">            self.ks_layers = []</span><br><span class="line">            self.vs_layers = []</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=<span class="literal">False</span>)))</span><br><span class="line">                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=<span class="literal">False</span>)))</span><br><span class="line">                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=<span class="literal">False</span>)))</span><br><span class="line">        self.attention = ScaledDotProductAttention(d_model)</span><br><span class="line">        self.layer_norm = LayerNormalization() <span class="keyword">if</span> use_norm <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.w_o = TimeDistributed(Dense(d_model))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        d_k, d_v = self.d_k, self.d_v</span><br><span class="line">        n_head = self.n_head</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="number">0</span>:</span><br><span class="line">            qs = self.qs_layer(q)  <span class="comment"># [batch_size, len_q, n_head*d_k]</span></span><br><span class="line">            ks = self.ks_layer(k)</span><br><span class="line">            vs = self.vs_layer(v)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">reshape1</span>(<span class="params">x</span>):</span><br><span class="line">                s = tf.shape(x)   <span class="comment"># [batch_size, len_q, n_head * d_k]</span></span><br><span class="line">                x = tf.reshape(x, [s[<span class="number">0</span>], s[<span class="number">1</span>], n_head, d_k])</span><br><span class="line">                x = tf.transpose(x, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>])  </span><br><span class="line">                x = tf.reshape(x, [-<span class="number">1</span>, s[<span class="number">1</span>], d_k])  <span class="comment"># [n_head * batch_size, len_q, d_k]</span></span><br><span class="line">                <span class="keyword">return</span> x</span><br><span class="line">            qs = Lambda(reshape1)(qs)</span><br><span class="line">            ks = Lambda(reshape1)(ks)</span><br><span class="line">            vs = Lambda(reshape1)(vs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                mask = Lambda(<span class="keyword">lambda</span> x:K.repeat_elements(x, n_head, <span class="number">0</span>))(mask)</span><br><span class="line">            head, attn = self.attention(qs, ks, vs, mask=mask)  </span><br><span class="line">                </span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">reshape2</span>(<span class="params">x</span>):</span><br><span class="line">                s = tf.shape(x)   <span class="comment"># [n_head * batch_size, len_v, d_v]</span></span><br><span class="line">                x = tf.reshape(x, [n_head, -<span class="number">1</span>, s[<span class="number">1</span>], s[<span class="number">2</span>]]) </span><br><span class="line">                x = tf.transpose(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">                x = tf.reshape(x, [-<span class="number">1</span>, s[<span class="number">1</span>], n_head*d_v])  <span class="comment"># [batch_size, len_v, n_head * d_v]</span></span><br><span class="line">                <span class="keyword">return</span> x</span><br><span class="line">            head = Lambda(reshape2)(head)</span><br><span class="line">        <span class="keyword">elif</span> self.mode == <span class="number">1</span>:</span><br><span class="line">            heads = []; attns = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">                qs = self.qs_layers[i](q)   </span><br><span class="line">                ks = self.ks_layers[i](k) </span><br><span class="line">                vs = self.vs_layers[i](v) </span><br><span class="line">                head, attn = self.attention(qs, ks, vs, mask)</span><br><span class="line">                heads.append(head); attns.append(attn)</span><br><span class="line">            head = Concatenate()(heads) <span class="keyword">if</span> n_head &gt; <span class="number">1</span> <span class="keyword">else</span> heads[<span class="number">0</span>]</span><br><span class="line">            attn = Concatenate()(attns) <span class="keyword">if</span> n_head &gt; <span class="number">1</span> <span class="keyword">else</span> attns[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        outputs = self.w_o(head)</span><br><span class="line">        outputs = Dropout(self.dropout)(outputs)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.layer_norm: <span class="keyword">return</span> outputs, attn</span><br><span class="line">        <span class="comment"># outputs = Add()([outputs, q]) # sl: fix</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(outputs), attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_hid, d_inner_hid, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        self.w_1 = Conv1D(d_inner_hid, <span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.w_2 = Conv1D(d_hid, <span class="number">1</span>)</span><br><span class="line">        self.layer_norm = LayerNormalization()</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.w_1(x) </span><br><span class="line">        output = self.w_2(output)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = Add()([output, x])</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, enc_input, mask=<span class="literal">None</span></span>):</span><br><span class="line">        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)</span><br><span class="line">        output = self.pos_ffn_layer(output)</span><br><span class="line">        <span class="keyword">return</span> output, slf_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GetPosEncodingMatrix</span>(<span class="params">max_len, d_emb</span>):</span><br><span class="line">    pos_enc = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * (j // <span class="number">2</span>) / d_emb) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d_emb)] </span><br><span class="line">        <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(d_emb) </span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_len)</span><br><span class="line">            ])</span><br><span class="line">    pos_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(pos_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># dim 2i</span></span><br><span class="line">    pos_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(pos_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>]) <span class="comment"># dim 2i+1</span></span><br><span class="line">    <span class="keyword">return</span> pos_enc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GetPadMask</span>(<span class="params">q, k</span>):</span><br><span class="line">    ones = K.expand_dims(K.ones_like(q, <span class="string">&#x27;float32&#x27;</span>), -<span class="number">1</span>)</span><br><span class="line">    mask = K.cast(K.expand_dims(K.not_equal(k, <span class="number">0</span>), <span class="number">1</span>), <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    mask = K.batch_dot(ones, mask, axes=[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GetSubMask</span>(<span class="params">s</span>):</span><br><span class="line">    len_s = tf.shape(s)[<span class="number">1</span>]</span><br><span class="line">    bs = tf.shape(s)[:<span class="number">1</span>]</span><br><span class="line">    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>

<pre><code>No module named &#39;dataloader&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">CnnTransformerModel</span>():</span><br><span class="line">    i = Input(shape = (<span class="number">150000</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=-<span class="number">1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(i)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D( <span class="number">8</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=-<span class="number">1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">16</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=-<span class="number">1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">32</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=-<span class="number">1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">64</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Bidirectional(LSTM(<span class="number">128</span>, return_sequences = <span class="literal">True</span>, return_state = <span class="literal">False</span>))(x)</span><br><span class="line">    </span><br><span class="line">    x = Bidirectional(LSTM(<span class="number">64</span>, return_sequences = <span class="literal">True</span>, return_state = <span class="literal">False</span>))(x)</span><br><span class="line">    </span><br><span class="line">    x, slf_attn = MultiHeadAttention(n_head=<span class="number">5</span>, d_model=<span class="number">300</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.3</span>)(x, x, x)</span><br><span class="line">    </span><br><span class="line">    avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">    </span><br><span class="line">    avg_pool = Dense(<span class="number">60</span>,activation = <span class="string">&#x27;relu&#x27;</span>)(avg_pool)</span><br><span class="line">    </span><br><span class="line">    y = Dense(<span class="number">1</span>,activation = <span class="string">&#x27;relu&#x27;</span>)(avg_pool)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Model(inputs = [i], outputs = [y])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = CnnTransformerModel()</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics = [<span class="string">&#x27;mean_absolute_error&#x27;</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 150000, 1)]  0                                            
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 150000, 1)    4           input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 15000, 8)     88          batch_normalization[0][0]        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 15000, 8)     32          conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 1500, 16)     1296        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 150, 32)      5152        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 150, 32)      128         conv1d_2[0][0]                   
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 15, 64)       20544       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, 15, 256)      197632      conv1d_3[0][0]                   
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 15, 128)      164352      bidirectional[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 15, 320)      40960       bidirectional_1[0][0]            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 15, 320)      40960       bidirectional_1[0][0]            
__________________________________________________________________________________________________
lambda (Lambda)                 (None, None, 64)     0           dense[0][0]                      
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, None, 64)     0           dense_1[0][0]                    
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, None)   0           lambda[0][0]                     
                                                                 lambda_1[0][0]                   
__________________________________________________________________________________________________
activation (Activation)         (None, None, None)   0           lambda_3[0][0]                   
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 15, 320)      40960       bidirectional_1[0][0]            
__________________________________________________________________________________________________
dropout (Dropout)               (None, None, None)   0           activation[0][0]                 
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, None, 64)     0           dense_2[0][0]                    
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, None, 64)     0           dropout[0][0]                    
                                                                 lambda_2[0][0]                   
__________________________________________________________________________________________________
lambda_5 (Lambda)               (None, None, 320)    0           lambda_4[0][0]                   
__________________________________________________________________________________________________
time_distributed (TimeDistribut (None, None, 300)    96300       lambda_5[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, None, 300)    0           time_distributed[0][0]           
__________________________________________________________________________________________________
layer_normalization (LayerNorma (None, None, 300)    600         dropout_1[0][0]                  
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 300)          0           layer_normalization[0][0]        
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 60)           18060       global_average_pooling1d[0][0]   
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            61          dense_4[0][0]                    
==================================================================================================
Total params: 627,193
Trainable params: 627,079
Non-trainable params: 114
__________________________________________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from IPython.display import SVG</span></span><br><span class="line"><span class="comment"># from keras.utils.vis_utils import model_to_dot</span></span><br><span class="line"><span class="comment"># SVG(model_to_dot(model,show_shapes = True).create(prog=&#x27;dot&#x27;, format=&#x27;svg&#x27;))</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start_time = time.time()</span><br><span class="line">hist = model.fit(</span><br><span class="line">    train_gen,</span><br><span class="line">    epochs = <span class="number">25</span>, </span><br><span class="line">    verbose = <span class="number">1</span>,validation_data= valid_gen</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- %s seconds ---&quot;</span> % (time.time() - start_time))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/25
400/400 [==============================] - 92s 230ms/step - loss: 8.1259 - mean_absolute_error: 2.1690 - val_loss: 8.6962 - val_mean_absolute_error: 2.2990
Epoch 2/25
400/400 [==============================] - 93s 232ms/step - loss: 6.8734 - mean_absolute_error: 1.9745 - val_loss: 10.5941 - val_mean_absolute_error: 2.5199
Epoch 3/25
400/400 [==============================] - 92s 231ms/step - loss: 6.2273 - mean_absolute_error: 1.8582 - val_loss: 8.3263 - val_mean_absolute_error: 2.1892
Epoch 4/25
400/400 [==============================] - 93s 232ms/step - loss: 6.3129 - mean_absolute_error: 1.8750 - val_loss: 7.2582 - val_mean_absolute_error: 2.1842
Epoch 5/25
400/400 [==============================] - 93s 232ms/step - loss: 6.1699 - mean_absolute_error: 1.8555 - val_loss: 10.6914 - val_mean_absolute_error: 2.5315
Epoch 6/25
400/400 [==============================] - 93s 232ms/step - loss: 6.3945 - mean_absolute_error: 1.8601 - val_loss: 17.6278 - val_mean_absolute_error: 3.4488
Epoch 7/25
400/400 [==============================] - 91s 228ms/step - loss: 5.9358 - mean_absolute_error: 1.7905 - val_loss: 7.1590 - val_mean_absolute_error: 2.0332
Epoch 8/25
400/400 [==============================] - 91s 227ms/step - loss: 5.8317 - mean_absolute_error: 1.7861 - val_loss: 7.4188 - val_mean_absolute_error: 2.0401
Epoch 9/25
400/400 [==============================] - 90s 226ms/step - loss: 6.0664 - mean_absolute_error: 1.8154 - val_loss: 11.4744 - val_mean_absolute_error: 2.6900
Epoch 10/25
400/400 [==============================] - 91s 228ms/step - loss: 5.6385 - mean_absolute_error: 1.7225 - val_loss: 6.3991 - val_mean_absolute_error: 1.9165
Epoch 11/25
400/400 [==============================] - 91s 228ms/step - loss: 5.7589 - mean_absolute_error: 1.7666 - val_loss: 14.0191 - val_mean_absolute_error: 2.8988
Epoch 12/25
400/400 [==============================] - 92s 229ms/step - loss: 5.3819 - mean_absolute_error: 1.6724 - val_loss: 7.1035 - val_mean_absolute_error: 2.1311
Epoch 13/25
400/400 [==============================] - 91s 227ms/step - loss: 5.5961 - mean_absolute_error: 1.7425 - val_loss: 14.4304 - val_mean_absolute_error: 2.9596
Epoch 14/25
400/400 [==============================] - 91s 227ms/step - loss: 5.3910 - mean_absolute_error: 1.6667 - val_loss: 7.3332 - val_mean_absolute_error: 2.1058
Epoch 15/25
400/400 [==============================] - 91s 228ms/step - loss: 5.3278 - mean_absolute_error: 1.6476 - val_loss: 8.5243 - val_mean_absolute_error: 2.1570
Epoch 16/25
400/400 [==============================] - 91s 227ms/step - loss: 5.1909 - mean_absolute_error: 1.6277 - val_loss: 10.1045 - val_mean_absolute_error: 2.4271
Epoch 17/25
400/400 [==============================] - 91s 228ms/step - loss: 5.5173 - mean_absolute_error: 1.6810 - val_loss: 7.0249 - val_mean_absolute_error: 2.0507
Epoch 18/25
400/400 [==============================] - 91s 228ms/step - loss: 4.8198 - mean_absolute_error: 1.5651 - val_loss: 9.4390 - val_mean_absolute_error: 2.4027
Epoch 19/25
400/400 [==============================] - 91s 228ms/step - loss: 5.0561 - mean_absolute_error: 1.6042 - val_loss: 6.4782 - val_mean_absolute_error: 1.9549
Epoch 20/25
400/400 [==============================] - 91s 227ms/step - loss: 4.9979 - mean_absolute_error: 1.5990 - val_loss: 7.7401 - val_mean_absolute_error: 2.0900
Epoch 21/25
400/400 [==============================] - 91s 227ms/step - loss: 5.0527 - mean_absolute_error: 1.6058 - val_loss: 12.2271 - val_mean_absolute_error: 2.7062
Epoch 22/25
400/400 [==============================] - 91s 227ms/step - loss: 4.1005 - mean_absolute_error: 1.4261 - val_loss: 7.2351 - val_mean_absolute_error: 2.0184
Epoch 23/25
400/400 [==============================] - 91s 227ms/step - loss: 5.1156 - mean_absolute_error: 1.6122 - val_loss: 6.6283 - val_mean_absolute_error: 1.9421
Epoch 24/25
400/400 [==============================] - 91s 228ms/step - loss: 5.0533 - mean_absolute_error: 1.6155 - val_loss: 14.7405 - val_mean_absolute_error: 3.0259
Epoch 25/25
400/400 [==============================] - 91s 228ms/step - loss: 5.0501 - mean_absolute_error: 1.6213 - val_loss: 12.0591 - val_mean_absolute_error: 2.6012
--- 2297.5507295131683 seconds ---
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(hist.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model MSE / Loss&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MSE/Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Test&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/12/06/kaggle-try/output_18_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(hist.history[<span class="string">&#x27;mean_absolute_error&#x27;</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">&#x27;val_mean_absolute_error&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model Mean Absolute Error&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MAE&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Test&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/12/06/kaggle-try/output_19_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">del</span> train_gen</span><br><span class="line"><span class="keyword">del</span> valid_gen</span><br><span class="line"><span class="keyword">del</span> X_train</span><br><span class="line"><span class="keyword">del</span> y_train</span><br><span class="line"><span class="keyword">del</span> train_df</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure>




<pre><code>6378
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_weights(<span class="string">&#x27;./trained_model.h5&#x27;</span>, overwrite=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, makedirs</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> isfile, join, basename, splitext, isfile, exists</span><br></pre></td></tr></table></figure>

<h2 id="Test-data-Normalize"><a href="#Test-data-Normalize" class="headerlink" title="Test data Normalize"></a>Test data Normalize</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_test</span>(<span class="params">ts_length = <span class="number">150000</span></span>):</span><br><span class="line">    base_dir = <span class="string">&#x27;./test/&#x27;</span></span><br><span class="line">    test_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> listdir(base_dir) <span class="keyword">if</span> isfile(join(base_dir, f))]</span><br><span class="line"></span><br><span class="line">    ts = np.empty([<span class="built_in">len</span>(test_files), ts_length])</span><br><span class="line">    ids = []</span><br><span class="line">    </span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> tqdm(test_files):</span><br><span class="line">        ids.append(splitext(f)[<span class="number">0</span>])</span><br><span class="line">        t_df = pd.read_csv(base_dir + f, dtype=&#123;<span class="string">&quot;acoustic_data&quot;</span>: np.int8&#125;)</span><br><span class="line">        ts[i, :] = t_df[<span class="string">&#x27;acoustic_data&#x27;</span>].values</span><br><span class="line">        i = i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ts, ids</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data, test_ids = load_test()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 2624/2624 [00:27&lt;00:00, 97.07it/s]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_test = test_data</span><br><span class="line">X_test = np.expand_dims(X_test, <span class="number">2</span>)</span><br><span class="line">X_test.shape</span><br></pre></td></tr></table></figure>




<pre><code>(2624, 150000, 1)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">submission_df = pd.DataFrame(&#123;<span class="string">&#x27;seg_id&#x27;</span>: test_ids, <span class="string">&#x27;time_to_failure&#x27;</span>: y_pred[:, <span class="number">0</span>]&#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">submission_df.to_csv(<span class="string">&quot;submission.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="후기"><a href="#후기" class="headerlink" title="후기"></a>후기</h2><p>  <img src="/2020/12/06/kaggle-try/kaggle_1.PNG" alt="greem"></p>
<blockquote>
<p>쉽지 않았던 도전…</p>
</blockquote>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/Study/page/0/">이전</a></div><div class="pagination-next"><a href="/categories/Study/page/2/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/Study/">1</a></li><li><a class="pagination-link" href="/categories/Study/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Study/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="ikarus-999"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">ikarus-999</p><p class="is-size-6 is-block">ikarus-999 BLOG</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nyan@null/null</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ikarus-999" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ikarus-999"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/GitPage/"><span class="level-start"><span class="level-item">GitPage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/GitPage/Hexo/"><span class="level-start"><span class="level-item">Hexo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">24</span></span></a><ul><li><a class="level is-mobile" href="/categories/Study/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Dacon/"><span class="level-start"><span class="level-item">Dacon</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Kaggle-Dacon/"><span class="level-start"><span class="level-item">Kaggle, Dacon</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Papers/"><span class="level-start"><span class="level-item">Papers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ilsang/"><span class="level-start"><span class="level-item">ilsang</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T07:26:50.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/bbackcheem/">bbackcheem</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-09T12:34:32.000Z">2021-09-09</time></p><p class="title"><a href="/2021/09/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0-2021/">인공지능 경진대회 후기_2021</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/Competition/">Competition</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-06T05:54:41.000Z">2021-09-06</time></p><p class="title"><a href="/2021/09/06/Multi-GPU/">Multi-GPU</a></p><p class="categories"><a href="/categories/Study/">Study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-15T10:43:57.000Z">2021-07-15</time></p><p class="title"><a href="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a></p><p class="categories"><a href="/categories/Study/">Study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-06-19T23:28:46.000Z">2021-06-20</time></p><p class="title"><a href="/2021/06/20/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/">게임개발-일기2</a></p><p class="categories"><a href="/categories/Game/">Game</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">링크</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://paperswithcode.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">paper</span></span><span class="level-right"><span class="level-item tag">paperswithcode.com</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">4월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">9월 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">7월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/06/"><span class="level-start"><span class="level-item">6월 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">2월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">1월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">12월 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">11월 2020</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Audio/"><span class="tag">Audio</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dacon/"><span class="tag">Dacon</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev/"><span class="tag">Dev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HEXO/"><span class="tag">HEXO</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kaggle/"><span class="tag">Kaggle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Music-EDA/"><span class="tag">Music EDA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Script/"><span class="tag">Script</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TF2/"><span class="tag">TF2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow2/"><span class="tag">Tensorflow2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Try/"><span class="tag">Try</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unity/"><span class="tag">Unity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XR/"><span class="tag">XR</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/blog/"><span class="tag">blog</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/coding/"><span class="tag">coding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/competition/"><span class="tag">competition</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/rules/"><span class="tag">rules</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unity/"><span class="tag">unity</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="ikarus&#039;s BLOG" height="28"></a><p class="is-size-7"><span>&copy; 2022 ikarus-999</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>